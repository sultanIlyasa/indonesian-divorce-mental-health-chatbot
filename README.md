# ğŸ§  Indonesian Divorce Mental Health Chatbot
**Transformer from Scratch â€“ TensorFlow + Gradio**

![Python](https://img.shields.io/badge/Python-3.9+-blue.svg)  
![TensorFlow](https://img.shields.io/badge/TensorFlow-2.x-orange.svg)  
![Gradio](https://img.shields.io/badge/Gradio-Web%20UI-green.svg)  
![License](https://img.shields.io/badge/License-MIT-lightgrey.svg)  

A web-based chatbot application built entirely from scratch using the Transformer architecture (Encoder-Decoder) in TensorFlow.  
The chatbot provides empathetic support for **children affected by parental divorce**, trained on a custom Indonesian dataset based on **John Bowlbyâ€™s Positive Attachment Theory**.

---

## ğŸ“Œ Features
- âœ… Transformer Encoder-Decoder built **from scratch** (no pre-trained models)
- âœ… Custom **Indonesian tokenizer** with `SubwordTextEncoder`
- âœ… Empathetic & context-aware responses based on **attachment theory**
- âœ… **Multi-tab UI**: Chat interface + Development info page
- âœ… Custom avatars, privacy-friendly design
- âœ… Evaluation with **BLEU & METEOR** metrics

---

## ğŸ›  Tech Stack
- **Language:** Python 3.9+
- **Frameworks/Libraries:** TensorFlow 2.x, TensorFlow Datasets, Gradio, NumPy, Matplotlib
- **Interface:** Gradio Blocks + ChatInterface
- **Model Type:** Transformer Encoder-Decoder

---

## ğŸ— Architecture Overview
```text
Data â†’ Tokenizer â†’ Positional Encoding â†’ Encoder Layers â†’ Decoder Layers
â†’ Multi-Head Attention â†’ Dense Output â†’ Response Generation

```

---
## Installation
- git clone https://github.com/yourusername/mental-health-chatbot-transformer.git
- cd mental-health-chatbot-transformer
- pip install -r requirements.txt

---
## Usage
```text
python app.py
```

---
## ğŸ“‚ Dataset
- Size: 525 conversational pairs
- Categories: Opening statements, parental context, termination
- Based on: John Bowlbyâ€™s Positive Attachment Theory
- Includes professional help triggers for high-risk cases

---
## âš™ How It Works
- User input â†’ tokenized & padded â†’ START/END tokens added
- Transformer model generates output via autoregressive decoding
- Masking ensures no future token or padding leakage in attention
- Final output is detokenized back into natural language

---
## Evaluation
- BLEU Score â€“ Token-level similarity with reference responses
- METEOR Score â€“ Semantic similarity at word-level
- Qualitative Review â€“ Checked for empathy & context accuracy
